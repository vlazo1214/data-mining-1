---
title: "hw2"
output: html_document
---

1. The null hypothesis in this scenario is the amount of TV, newspaper, and radio advertising
 is not associated with the amount of sales. A significant p-value in any of these cases
 would show that the associated predictor (TV, newspaper, or radio) has a statistically
 significant effect on the response (sales). Based on the table, TV and radio advertisements
 are significant on the amount of sales while spending money on newspaper advertisements are not, causing 
 a rejection of their respective null hypotheses.

2. The KNN classifier refers to the algorithm that predicts a new data point's class
by finding the K nearest data points and labeling it with the most common class between it and those data
points. The KNN regression method is a form of regression that takes a new data point, x_i,
compares it to the K nearest data points, and predicts the response that x_i would yield
by taking the averages of responses of the K nearest data points. The classifier simply
predicts the category that a new data point would be in while the regression method predicts
a new data point's response.


3. Salary prediction
(a) Correct answer: iv. For a fixed value of IQ and GPA, college graduates earn
more, on average, than high school graduates provided that
the GPA is high enough.

(b) Answer: 137.1

Let `x_1` be GPA, `x_2` be IQ, `x_3` be level, `x_4` be GPA * IQ, and `x_5` be GPA * level, so:
`x_1 = 4, x_2 = 110, x_3 = 1, x_4 = 440, x_5 = 4`.
Substituting:
50 + 20 * 4 + 0.07 * 110 + 35 * 1 + 0.01 * 440 + (−10 * 4) = 137.1

(c) False, a coefficient's value is does not explicitly denote whether it is statistically significant.

4. I collect a set of data (n = 100 observations) containing a single
predictor and a quantitative response. I then fit a linear regression
model to the data, as well as a separate cubic regression, i.e. Y =
β0 + β1X + β2X2 + β3X3 + ϵ.

(a) The training RSS for the cubic regression model would be lower than the linear regression. 
This is because the cubic model is more flexible than the linear model and is more
well-suited to fit around noise than the linear model.

(b) For test, the linear model would perform better (have lower error) than the cubic model.
With more flexibility, the cubic model lost stability and overfit on the training data, causing
a higher error. The linear model is more stable so there is no increase in error given the 
true linear relationship between X and Y.

(c) The training RSS for the cubic regression model would be lower than the linear regression. 
This is because the cubic model can capture non-linear patterns in data than an inflexible
model such as the linear model.
(d) In general, the cubic model is able to yield a lower test RSS if the true relationship between X and Y
 is non-linear. The linear model has more of a chance of underfitting but it depends on how much
 the true relationship is non-linear.

14. This problem focuses on the collinearity problem.
(a) Perform the following commands in R:
```{r}
set.seed(1)
x1 <- runif(100)
x2 <- 0.5 * x1 + rnorm(100) / 10
y <- 2 + 2 * x1 + 0.3 * x2 + rnorm(100)
```

The last line corresponds to creating a linear model in which y is
a function of x1 and x2. Write out the form of the linear model.
What are the regression coefficients?

y = B_0 + B_1 * x1 + B_2 * x2 + e 

The regression coefficients are B_1 and B_2 that correspond with x1 and x2, respectively.

(b) What is the correlation between x1 and x2? Create a scatterplot
displaying the relationship between the variables.

x2 halves x1's values and adds noise represented by 100 points that follow a normal distribution
and dividing them by 10. This scales down x1's values and creates a linear relationship:

```{r}
cor(x1, x2)
plot(x1, x2, main="Relationship between x1 and x2")
```

(c) Using this data, fit a least squares regression to predict y using
x1 and x2. Describe the results obtained. What are βˆ0, βˆ1, and
βˆ2? How do these relate to the true β0, β1, and β2? Can you
reject the null hypothesis H0 : β1 = 0? How about the null
hypothesis H0 : β2 = 0?

```{r}
model <- lm(y ~ x1 + x2)
summary(model)
```
We can see the Multiple R-squared, 0.2088 which means that ~20% of the data can be explained by the model.
B_0 is 2.1305, B_1 is 1.4396, and B_2 is 1.0097. The true B_0 is the intercept of the 
line and B_1 and B_2 are the coefficients of the model. For B_1, we can reject
the null hypothesis of B_1 = 0 given the significant p-value. For B_2, we fail to reject
the null hypothesis of B_2 = 0 because there is no significant p-value.

(d) Now fit a least squares regression to predict y using only x1.
Comment on your results. Can you reject the null hypothesis
H0 : β1 = 0?

```{r}
x1_model = lm(y ~ x1)
summary(x1_model)
```
We can see the Multiple R-squared, 0.2088 which means that ~20% of the data can be explained by the new model. We
also can see that x1's p-value is significantly smaller than in the previous model. This
means that we still reject a null hypothesis of B_1 = 0.

(e) Now fit a least squares regression to predict y using only x2.
Comment on your results. Can you reject the null hypothesis
H0 : β1 = 0?

```{r}
x2_model = lm(y ~ x2)
summary(x2_model)
```

We can see the Multiple R-squared, 0.1763 which means that ~17% of the data can be explained by the new model. We
also can see that x2's p-value is significantly smaller than in the first model, making it significant.
In this case, we reject a null hypothesis of B_2 = 0.

(f) Do the results obtained in (c)–(e) contradict each other? Explain
your answer.
(g) Now suppose we obtain one additional observation, which was
unfortunately mismeasured.

```
x1 <- c(x1, 0.1)
x2 <- c(x2, 0.8)
y <- c(y, 6)
```

Re-fit the linear models from (c) to (e) using this new data. What
effect does this new observation have on the each of the models?
In each model, is this observation an outlier? A high-leverage
point? Both? Explain your answers.