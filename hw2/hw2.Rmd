---
title: "hw2"
output: html_document
---

1. Describe the null hypotheses to which the p-values given in Table 3.4
correspond. Explain what conclusions you can draw based on these
p-values. Your explanation should be phrased in terms of sales, TV,
radio, and newspaper, rather than in terms of the coefficients of the
linear model.

The null hypothesis in this scenario is the amount of TV, newspaper, and radio advertising
 is not associated with the amount of sales. A significant p-value in any of these cases
 would show that the associated predictor (TV, newspaper, or radio) has a statistically
 significant effect on the response (sales). Based on the table, TV and radio advertisements
 are significant on the amount of sales while spending money on newspaper advertisements are not, causing 
 a rejection of their respective null hypotheses.

2. Carefully explain the differences between the KNN classifier and KNN
regression methods.

The KNN classifier refers to the algorithm that predicts a new data point's class
by finding the K nearest data points and labeling it with the most common class between it and those data
points. The KNN regression method is a form of regression that takes a new data point, x_i,
compares it to the K nearest data points, and predicts the response that x_i would yield
by taking the averages of responses of the K nearest data points. The classifier simply
predicts the category that a new data point would be in while the regression method predicts
a new data point's response.


3. Salary prediction
(a) Correct answer: iv. For a fixed value of IQ and GPA, college graduates earn
more, on average, than high school graduates provided that
the GPA is high enough.

(b) Answer: 137.1

Let `x_1` be GPA, `x_2` be IQ, `x_3` be level, `x_4` be GPA * IQ, and `x_5` be GPA * level, so:
`x_1 = 4, x_2 = 110, x_3 = 1, x_4 = 440, x_5 = 4`.
Substituting:
50 + 20 * 4 + 0.07 * 110 + 35 * 1 + 0.01 * 440 + (−10 * 4) = 137.1

(c) False, a coefficient's value is does not explicitly denote whether it is statistically significant.

4. I collect a set of data (n = 100 observations) containing a single
predictor and a quantitative response. I then fit a linear regression
model to the data, as well as a separate cubic regression, i.e. Y =
β0 + β1X + β2X2 + β3X3 + ϵ.
(a) Suppose that the true relationship between X and Y is linear,
i.e. Y = β0 + β1X + ϵ. Consider the training residual sum of
squares (RSS) for the linear regression, and also the training
RSS for the cubic regression. Would we expect one to be lower
than the other, would we expect them to be the same, or is there
not enough information to tell? Justify your answer.
(b) Answer (a) using test rather than training RSS.
(c) Suppose that the true relationship between X and Y is not linear,
but we don’t know how far it is from linear. Consider the training
RSS for the linear regression, and also the training RSS for the
cubic regression. Would we expect one to be lower than the
other, would we expect them to be the same, or is there not
enough information to tell? Justify your answer.
(d) Answer (c) using test rather than training RSS.

14. This problem focuses on the collinearity problem.
(a) Perform the following commands in R:
```r{}
> set.seed(1)
> x1 <- runif(100)
> x2 <- 0.5 * x1 + rnorm(100) / 10
> y <- 2 + 2 * x1 + 0.3 * x2 + rnorm(100)
```

The last line corresponds to creating a linear model in which y is
a function of x1 and x2. Write out the form of the linear model.
What are the regression coefficients?
(b) What is the correlation between x1 and x2? Create a scatterplot
displaying the relationship between the variables.

(c) Using this data, fit a least squares regression to predict y using
x1 and x2. Describe the results obtained. What are βˆ0, βˆ1, and
βˆ2? How do these relate to the true β0, β1, and β2? Can you
reject the null hypothesis H0 : β1 = 0? How about the null
hypothesis H0 : β2 = 0?

(d) Now fit a least squares regression to predict y using only x1.
Comment on your results. Can you reject the null hypothesis
H0 : β1 = 0?

(e) Now fit a least squares regression to predict y using only x2.
Comment on your results. Can you reject the null hypothesis
H0 : β1 = 0?
128 3. Linear Regression
(f) Do the results obtained in (c)–(e) contradict each other? Explain
your answer.
(g) Now suppose we obtain one additional observation, which was
unfortunately mismeasured.

```r{}
> x1 <- c(x1, 0.1)
> x2 <- c(x2, 0.8)
> y <- c(y, 6)
```

Re-fit the linear models from (c) to (e) using this new data. What
effect does this new observation have on the each of the models?
In each model, is this observation an outlier? A high-leverage
point? Both? Explain your answers.